{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e629e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/9mxt04xj57qg0q48b2b677cm0000gn/T/ipykernel_48698/4118202117.py:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:                                   URL  URLLength                      Domain  \\\n",
      "0    https://www.southbankmosaics.com         31    www.southbankmosaics.com   \n",
      "1            https://www.uni-mainz.de         23            www.uni-mainz.de   \n",
      "2      https://www.voicefmradio.co.uk         29      www.voicefmradio.co.uk   \n",
      "3         https://www.sfnmjournal.com         26         www.sfnmjournal.com   \n",
      "4  https://www.rewildingargentina.org         33  www.rewildingargentina.org   \n",
      "\n",
      "   DomainLength  IsDomainIP  TLD  URLSimilarityIndex  CharContinuationRate  \\\n",
      "0            24           0  com               100.0              1.000000   \n",
      "1            16           0   de               100.0              0.666667   \n",
      "2            22           0   uk               100.0              0.866667   \n",
      "3            19           0  com               100.0              1.000000   \n",
      "4            26           0  org               100.0              1.000000   \n",
      "\n",
      "   TLDLegitimateProb  URLCharProb  ...  Pay  Crypto  HasCopyrightInfo  \\\n",
      "0           0.522907     0.061933  ...    0       0                 1   \n",
      "1           0.032650     0.050207  ...    0       0                 1   \n",
      "2           0.028555     0.064129  ...    0       0                 1   \n",
      "3           0.522907     0.057606  ...    1       1                 1   \n",
      "4           0.079963     0.059441  ...    1       0                 1   \n",
      "\n",
      "   NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  \\\n",
      "0         34       20      28          119             0              124   \n",
      "1         50        9       8           39             0              217   \n",
      "2         10        2       7           42             2                5   \n",
      "3          3       27      15           22             1               31   \n",
      "4        244       15      34           72             1               85   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"PhiUSIIL_Phishing_URL_Dataset.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"ndarvind/phiusiil-phishing-url-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db85875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "0: URL\n",
      "1: URLLength\n",
      "2: Domain\n",
      "3: DomainLength\n",
      "4: IsDomainIP\n",
      "5: TLD\n",
      "6: URLSimilarityIndex\n",
      "7: CharContinuationRate\n",
      "8: TLDLegitimateProb\n",
      "9: URLCharProb\n",
      "10: TLDLength\n",
      "11: NoOfSubDomain\n",
      "12: HasObfuscation\n",
      "13: NoOfObfuscatedChar\n",
      "14: ObfuscationRatio\n",
      "15: NoOfLettersInURL\n",
      "16: LetterRatioInURL\n",
      "17: NoOfDegitsInURL\n",
      "18: DegitRatioInURL\n",
      "19: NoOfEqualsInURL\n",
      "20: NoOfQMarkInURL\n",
      "21: NoOfAmpersandInURL\n",
      "22: NoOfOtherSpecialCharsInURL\n",
      "23: SpacialCharRatioInURL\n",
      "24: IsHTTPS\n",
      "25: LineOfCode\n",
      "26: LargestLineLength\n",
      "27: HasTitle\n",
      "28: Title\n",
      "29: DomainTitleMatchScore\n",
      "30: URLTitleMatchScore\n",
      "31: HasFavicon\n",
      "32: Robots\n",
      "33: IsResponsive\n",
      "34: NoOfURLRedirect\n",
      "35: NoOfSelfRedirect\n",
      "36: HasDescription\n",
      "37: NoOfPopup\n",
      "38: NoOfiFrame\n",
      "39: HasExternalFormSubmit\n",
      "40: HasSocialNet\n",
      "41: HasSubmitButton\n",
      "42: HasHiddenFields\n",
      "43: HasPasswordField\n",
      "44: Bank\n",
      "45: Pay\n",
      "46: Crypto\n",
      "47: HasCopyrightInfo\n",
      "48: NoOfImage\n",
      "49: NoOfCSS\n",
      "50: NoOfJS\n",
      "51: NoOfSelfRef\n",
      "52: NoOfEmptyRef\n",
      "53: NoOfExternalRef\n",
      "54: label\n",
      "\n",
      "DataFrame shape: (235795, 55)\n",
      "\n",
      "Data types:\n",
      "URL                            object\n",
      "URLLength                       int64\n",
      "Domain                         object\n",
      "DomainLength                    int64\n",
      "IsDomainIP                      int64\n",
      "TLD                            object\n",
      "URLSimilarityIndex            float64\n",
      "CharContinuationRate          float64\n",
      "TLDLegitimateProb             float64\n",
      "URLCharProb                   float64\n",
      "TLDLength                       int64\n",
      "NoOfSubDomain                   int64\n",
      "HasObfuscation                  int64\n",
      "NoOfObfuscatedChar              int64\n",
      "ObfuscationRatio              float64\n",
      "NoOfLettersInURL                int64\n",
      "LetterRatioInURL              float64\n",
      "NoOfDegitsInURL                 int64\n",
      "DegitRatioInURL               float64\n",
      "NoOfEqualsInURL                 int64\n",
      "NoOfQMarkInURL                  int64\n",
      "NoOfAmpersandInURL              int64\n",
      "NoOfOtherSpecialCharsInURL      int64\n",
      "SpacialCharRatioInURL         float64\n",
      "IsHTTPS                         int64\n",
      "LineOfCode                      int64\n",
      "LargestLineLength               int64\n",
      "HasTitle                        int64\n",
      "Title                          object\n",
      "DomainTitleMatchScore         float64\n",
      "URLTitleMatchScore            float64\n",
      "HasFavicon                      int64\n",
      "Robots                          int64\n",
      "IsResponsive                    int64\n",
      "NoOfURLRedirect                 int64\n",
      "NoOfSelfRedirect                int64\n",
      "HasDescription                  int64\n",
      "NoOfPopup                       int64\n",
      "NoOfiFrame                      int64\n",
      "HasExternalFormSubmit           int64\n",
      "HasSocialNet                    int64\n",
      "HasSubmitButton                 int64\n",
      "HasHiddenFields                 int64\n",
      "HasPasswordField                int64\n",
      "Bank                            int64\n",
      "Pay                             int64\n",
      "Crypto                          int64\n",
      "HasCopyrightInfo                int64\n",
      "NoOfImage                       int64\n",
      "NoOfCSS                         int64\n",
      "NoOfJS                          int64\n",
      "NoOfSelfRef                     int64\n",
      "NoOfEmptyRef                    int64\n",
      "NoOfExternalRef                 int64\n",
      "label                           int64\n",
      "dtype: object\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "1    134850\n",
      "0    100945\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print all column names in the DataFrame\n",
    "print(\"Column names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i}: {col}\")\n",
    "\n",
    "# Display the shape of the DataFrame\n",
    "print(\"\\nDataFrame shape:\", df.shape)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83516b85",
   "metadata": {},
   "source": [
    "# Phishing URL Detection - Feature Analysis First Approach\n",
    "\n",
    "In this notebook, we'll analyze a dataset of URLs to identify which features are most effective for distinguishing between legitimate and phishing URLs. Only after determining the important features will we build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134eebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e905b",
   "metadata": {},
   "source": [
    "## 1. Exploring the Dataset\n",
    "\n",
    "Let's start by exploring the dataset to understand what features we have and how they're distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c03df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's look at the dataset shape and how many samples we have\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nFeatures with missing values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check class balance\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"Percentage of legitimate URLs: {df['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcad0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='label', data=df)\n",
    "plt.title('Distribution of Legitimate vs Phishing URLs')\n",
    "plt.xlabel('Class (1 = Legitimate, 0 = Phishing)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add count labels on top of the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():,}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38b6d3",
   "metadata": {},
   "source": [
    "## 2. Detailed Feature Analysis\n",
    "\n",
    "Let's analyze each feature in detail to understand its distribution and relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all features (excluding the URL itself and the label)\n",
    "features = [col for col in df.columns if col not in ['URL', 'label']]\n",
    "print(f\"Total number of features to analyze: {len(features)}\")\n",
    "\n",
    "# Let's create a function to analyze features in batches\n",
    "def analyze_feature_batch(feature_batch, df, rows=3, cols=3):\n",
    "    \"\"\"Create histograms for a batch of features, split by class\"\"\"\",\n",
    "    plt.figure(figsize=(18, 5*rows))\n",
    "    for i, feature in enumerate(feature_batch):\n",
    "        if i < rows*cols:\n",
    "            plt.subplot(rows, cols, i+1)\n",
    "            try:\n",
    "                sns.histplot(data=df, x=feature, hue='label', bins=30, kde=True)\n",
    "                plt.title(f'Distribution of {feature} by class')\n",
    "            except Exception as e:\n",
    "                print(f\"Error plotting {feature}: {e}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33649dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze URL-related features first (batch 1)\n",
    "url_features = [col for col in features if 'URL' in col or 'Domain' in col or 'TLD' in col]\n",
    "print(\"URL-related features:\")\n",
    "print(url_features)\n",
    "analyze_feature_batch(url_features, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9477a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze character-related features (batch 2)\n",
    "char_features = [col for col in features if 'Char' in col or 'Letter' in col or 'Degit' in col or 'Special' in col]\n",
    "print(\"Character-related features:\")\n",
    "print(char_features)\n",
    "analyze_feature_batch(char_features, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze web page content-related features (batch 3)\n",
    "page_features = [col for col in features if col not in url_features + char_features]\n",
    "print(\"Web page content-related features:\")\n",
    "print(page_features)\n",
    "analyze_feature_batch(page_features, df, rows=5, cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95dc698",
   "metadata": {},
   "source": [
    "## 3. Feature Correlation Analysis\n",
    "\n",
    "Let's identify which features are most correlated with our target variable (label) and examine relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with the target variable\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "correlations = df[numeric_features].corr()['label'].drop('label')\n",
    "\n",
    "# Display top positive and negative correlations\n",
    "print(\"Features most positively correlated with legitimate URLs:\")\n",
    "print(correlations.sort_values(ascending=False).head(10))\n",
    "print(\"\\nFeatures most negatively correlated with legitimate URLs (i.e., indicators of phishing):\")\n",
    "print(correlations.sort_values().head(10))\n",
    "\n",
    "# Visualize the correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=correlations.sort_values(ascending=False).head(15).values, \n",
    "            y=correlations.sort_values(ascending=False).head(15).index)\n",
    "plt.title('Top 15 Features Correlated with Legitimate URLs')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=correlations.sort_values().head(15).values, \n",
    "            y=correlations.sort_values().head(15).index)\n",
    "plt.title('Top 15 Features Correlated with Phishing URLs')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428edf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for a subset of important features\n",
    "important_features = list(correlations.abs().sort_values(ascending=False).head(15).index)\n",
    "important_features.append('label')  # Add the target variable\n",
    "\n",
    "# Create a correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "important_corr = df[important_features].corr()\n",
    "sns.heatmap(important_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Top Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b7012d",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Using Statistical Methods\n",
    "\n",
    "Let's further analyze feature importance using statistical methods to understand which features are most discriminative for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for feature selection\n",
    "X = df.drop(['URL', 'label'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Apply ANOVA F-test (works well for classification tasks)\n",
    "f_test = SelectKBest(f_classif, k='all')\n",
    "f_test.fit(X, y)\n",
    "\n",
    "# Get and display feature scores\n",
    "f_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F_score': f_test.scores_,\n",
    "    'p_value': f_test.pvalues_\n",
    "}).sort_values('F_score', ascending=False)\n",
    "\n",
    "print(\"Top 15 features based on ANOVA F-test:\")\n",
    "print(f_scores.head(15))\n",
    "\n",
    "# Visualize feature importance scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='F_score', y='Feature', data=f_scores.head(20))\n",
    "plt.title('Feature Importance from ANOVA F-test')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Mutual Information (another method to measure feature relevance)\n",
    "mi = SelectKBest(mutual_info_classif, k='all')\n",
    "mi.fit(X, y)\n",
    "\n",
    "# Get and display feature scores\n",
    "mi_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'MI_score': mi.scores_\n",
    "}).sort_values('MI_score', ascending=False)\n",
    "\n",
    "print(\"Top 15 features based on Mutual Information:\")\n",
    "print(mi_scores.head(15))\n",
    "\n",
    "# Visualize feature importance scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='MI_score', y='Feature', data=mi_scores.head(20))\n",
    "plt.title('Feature Importance from Mutual Information')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a8cea",
   "metadata": {},
   "source": [
    "## 5. URL-Only Feature Subset Analysis\n",
    "\n",
    "Given that our requirement is to classify URLs based only on the URL string (without accessing the webpage),\n",
    "let's identify which URL-related features are available in this dataset and how effective they would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URL-only features (features that can be extracted from the URL string without visiting the webpage)\n",
    "url_only_features = [\n",
    "    'URLLength', 'Domain', 'DomainLength', 'IsDomainIP', 'TLD', 'URLSimilarityIndex', \n",
    "    'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength', \n",
    "    'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', \n",
    "    'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', \n",
    "    'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', \n",
    "    'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS'\n",
    "]\n",
    "\n",
    "# Check which features are actually available in our dataset\n",
    "available_url_features = [f for f in url_only_features if f in df.columns]\n",
    "print(f\"Available URL-only features: {len(available_url_features)} out of {len(url_only_features)}\")\n",
    "print(available_url_features)\n",
    "\n",
    "# Extract the subset for further analysis\n",
    "url_features_df = df[available_url_features + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9081de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation of URL-only features with the target\n",
    "url_correlations = url_features_df.corr()['label'].drop('label')\n",
    "\n",
    "# Display top positive and negative correlations\n",
    "print(\"URL-only features most positively correlated with legitimate URLs:\")\n",
    "print(url_correlations.sort_values(ascending=False).head(10))\n",
    "print(\"\\nURL-only features most negatively correlated with legitimate URLs:\")\n",
    "print(url_correlations.sort_values().head(10))\n",
    "\n",
    "# Visualize the correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=url_correlations.abs().sort_values(ascending=False).values, \n",
    "            y=url_correlations.abs().sort_values(ascending=False).index)\n",
    "plt.title('URL-Only Features Ranked by Correlation Strength with Target')\n",
    "plt.xlabel('Absolute Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5efad",
   "metadata": {},
   "source": [
    "## 6. Tree-Based Feature Importance\n",
    "\n",
    "Let's use a tree-based method (Random Forest) to evaluate feature importance as it captures non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare URL-only feature data for Random Forest\n",
    "X_url = url_features_df.drop(['label'], axis=1)\n",
    "y_url = url_features_df['label']\n",
    "\n",
    "# Pre-process: handle non-numeric columns if any\n",
    "X_url_processed = pd.get_dummies(X_url)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_url_processed, y_url)\n",
    "\n",
    "# Get feature importances\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Feature': X_url_processed.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 URL-only features based on Random Forest importance:\")\n",
    "print(rf_importances.head(15))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=rf_importances.head(15))\n",
    "plt.title('URL-Only Feature Importance from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c6c397",
   "metadata": {},
   "source": [
    "## 7. Feature Selection for URL-Only Classification\n",
    "\n",
    "Based on our analysis, let's select the most important URL-only features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine insights from correlation analysis and Random Forest importance\n",
    "# Select top features from each method and combine them\n",
    "top_corr_features = list(url_correlations.abs().sort_values(ascending=False).head(10).index)\n",
    "top_rf_features = list(rf_importances.head(10)['Feature'])\n",
    "                                          \n",
    "# Combine unique features from both methods\n",
    "selected_features = list(set(top_corr_features + top_rf_features))\n",
    "print(f\"Selected features for URL-only classification: {len(selected_features)}\")\n",
    "print(selected_features)\n",
    "\n",
    "# Save these selected features for our model\n",
    "selected_url_features = selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c4027",
   "metadata": {},
   "source": [
    "## 8. Function to Extract URL Features\n",
    "\n",
    "Now that we know which features are important, let's create a function to extract these features from a URL string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extract key features from a URL string based on our feature importance analysis\"\"\"\n",
    "    \n",
    "    # Parse the URL\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "    except:\n",
    "        domain = \"\"\n",
    "    \n",
    "    # Basic URL features\n",
    "    url_length = len(url)\n",
    "    domain_length = len(domain)\n",
    "    \n",
    "    # TLD (Top-Level Domain)\n",
    "    tld = \"\"\n",
    "    if domain and \".\" in domain:\n",
    "        tld = domain.split(\".\")[-1]\n",
    "    tld_length = len(tld)\n",
    "    \n",
    "    # Count characters in URL\n",
    "    letter_count = sum(c.isalpha() for c in url)\n",
    "    letter_ratio = letter_count / url_length if url_length > 0 else 0\n",
    "    \n",
    "    digit_count = sum(c.isdigit() for c in url)\n",
    "    digit_ratio = digit_count / url_length if url_length > 0 else 0\n",
    "    \n",
    "    special_chars = sum(not c.isalnum() for c in url)\n",
    "    special_char_ratio = special_chars / url_length if url_length > 0 else 0\n",
    "    \n",
    "    # Special character counts\n",
    "    equals_count = url.count('=')\n",
    "    qmark_count = url.count('?')\n",
    "    ampersand_count = url.count('&')\n",
    "    other_special_chars = special_chars - (url.count('/') + equals_count + qmark_count + ampersand_count)\n",
    "    \n",
    "    # Subdomain analysis\n",
    "    subdomain_count = domain.count('.') if domain else 0\n",
    "    \n",
    "    # Security indicators\n",
    "    is_https = 1 if url.startswith(\"https://\") else 0\n",
    "    \n",
    "    # Check if domain is an IP address\n",
    "    ip_pattern = re.compile(r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$')\n",
    "    is_domain_ip = 1 if ip_pattern.match(domain) else 0\n",
    "    \n",
    "    # Return features as a dictionary\n",
    "    features = {\n",
    "        'URLLength': url_length,\n",
    "        'DomainLength': domain_length,\n",
    "        'TLDLength': tld_length,\n",
    "        'NoOfLettersInURL': letter_count,\n",
    "        'LetterRatioInURL': letter_ratio,\n",
    "        'NoOfDegitsInURL': digit_count,\n",
    "        'DegitRatioInURL': digit_ratio,\n",
    "        'NoOfEqualsInURL': equals_count,\n",
    "        'NoOfQMarkInURL': qmark_count, \n",
    "        'NoOfAmpersandInURL': ampersand_count,\n",
    "        'NoOfOtherSpecialCharsInURL': other_special_chars,\n",
    "        'SpacialCharRatioInURL': special_char_ratio,\n",
    "        'IsHTTPS': is_https,\n",
    "        'IsDomainIP': is_domain_ip,\n",
    "        'NoOfSubDomain': subdomain_count\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896e437",
   "metadata": {},
   "source": [
    "## 9. Model Building with Selected Features\n",
    "\n",
    "Now that we've identified the important features, let's build a model using only those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42960789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Create dataset with only the selected features\n",
    "X_selected = df[selected_url_features]\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest model with the selected features\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "y_prob = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability of being legitimate\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b108f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Use a smaller subset for grid search to save time\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train_scaled, y_train, \n",
    "                                                        test_size=0.7, random_state=42, stratify=y_train)\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), \n",
    "                          param_grid=param_grid, \n",
    "                          cv=3, \n",
    "                          n_jobs=-1,\n",
    "                          scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_y_pred = best_model.predict(X_test_scaled)\n",
    "print(f\"\\nTest accuracy with best model: {accuracy_score(y_test, best_y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, best_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c6adb",
   "metadata": {},
   "source": [
    "## 10. URL Classification Function\n",
    "\n",
    "Now let's create a function that can classify a new URL as legitimate or phishing, using only URL-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1427e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_url(url, model=best_model, scaler=scaler, selected_features=selected_url_features):\n",
    "    \"\"\"Classify a URL as legitimate (1) or phishing (0) using only URL-based features\"\"\"\n",
    "    \n",
    "    # Extract features from the URL\n",
    "    features = extract_url_features(url)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame([features])\n",
    "    \n",
    "    # Select only the features used by the model\n",
    "    # Handle any missing features that might not have been extracted\n",
    "    missing_features = set(selected_features) - set(features_df.columns)\n",
    "    for feature in missing_features:\n",
    "        features_df[feature] = 0  # Default value\n",
    "    \n",
    "    features_df = features_df[selected_features]\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    probability = model.predict_proba(features_scaled)[0][1]  # Probability of being legitimate\n",
    "    \n",
    "    # Interpret result\n",
    "    result = \"Legitimate\" if prediction == 1 else \"Phishing\"\n",
    "    confidence = probability if prediction == 1 else 1 - probability\n",
    "    \n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"prediction\": int(prediction),\n",
    "        \"result\": result,\n",
    "        \"confidence\": confidence,\n",
    "        \"extracted_features\": features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with some example URLs\n",
    "test_urls = [\n",
    "    \"https://www.google.com\",\n",
    "    \"https://www.microsoft.com\",\n",
    "    \"http://googl3-security.com\",\n",
    "    \"http://paypal-secure.com.verification.login.asp\",\n",
    "    \"https://github.com\"\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    result = classify_url(url)\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"Prediction: {result['result']} (Confidence: {result['confidence']:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4c1d5",
   "metadata": {},
   "source": [
    "## 11. Model Export\n",
    "\n",
    "Finally, let's save our model and associated components for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eecdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model and related objects\n",
    "joblib.dump(best_model, 'phishing_detector_model.pkl')\n",
    "joblib.dump(scaler, 'phishing_detector_scaler.pkl')\n",
    "joblib.dump(selected_url_features, 'phishing_detector_features.pkl')\n",
    "\n",
    "print(\"Model and related objects have been saved:\")\n",
    "print(\"- phishing_detector_model.pkl\")\n",
    "print(\"- phishing_detector_scaler.pkl\")\n",
    "print(\"- phishing_detector_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53446254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to load and use the saved model in a production environment\n",
    "def load_phishing_detector():\n",
    "    \"\"\"Load the trained phishing detector model and related objects\"\"\"\n",
    "    model = joblib.load('phishing_detector_model.pkl')\n",
    "    scaler = joblib.load('phishing_detector_scaler.pkl')\n",
    "    features = joblib.load('phishing_detector_features.pkl')\n",
    "    return model, scaler, features\n",
    "\n",
    "def predict_url(url, model=None, scaler=None, features=None):\n",
    "    \"\"\"Predict if a URL is phishing or legitimate using the saved model\"\"\"\n",
    "    # Load model if not provided\n",
    "    if model is None or scaler is None or features is None:\n",
    "        model, scaler, features = load_phishing_detector()\n",
    "    \n",
    "    # Extract features\n",
    "    url_features_dict = extract_url_features(url)\n",
    "    features_df = pd.DataFrame([url_features_dict])\n",
    "    features_df = features_df[features]  # Ensure correct order\n",
    "    \n",
    "    # Scale and predict\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    probability = model.predict_proba(features_scaled)[0][1]\n",
    "    \n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"is_legitimate\": bool(prediction),\n",
    "        \"confidence\": probability if prediction == 1 else 1 - probability\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
