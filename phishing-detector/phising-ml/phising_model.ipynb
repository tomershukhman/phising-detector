{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bc90abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/9mxt04xj57qg0q48b2b677cm0000gn/T/ipykernel_73865/4118202117.py:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:                                   URL  URLLength                      Domain  \\\n",
      "0    https://www.southbankmosaics.com         31    www.southbankmosaics.com   \n",
      "1            https://www.uni-mainz.de         23            www.uni-mainz.de   \n",
      "2      https://www.voicefmradio.co.uk         29      www.voicefmradio.co.uk   \n",
      "3         https://www.sfnmjournal.com         26         www.sfnmjournal.com   \n",
      "4  https://www.rewildingargentina.org         33  www.rewildingargentina.org   \n",
      "\n",
      "   DomainLength  IsDomainIP  TLD  URLSimilarityIndex  CharContinuationRate  \\\n",
      "0            24           0  com               100.0              1.000000   \n",
      "1            16           0   de               100.0              0.666667   \n",
      "2            22           0   uk               100.0              0.866667   \n",
      "3            19           0  com               100.0              1.000000   \n",
      "4            26           0  org               100.0              1.000000   \n",
      "\n",
      "   TLDLegitimateProb  URLCharProb  ...  Pay  Crypto  HasCopyrightInfo  \\\n",
      "0           0.522907     0.061933  ...    0       0                 1   \n",
      "1           0.032650     0.050207  ...    0       0                 1   \n",
      "2           0.028555     0.064129  ...    0       0                 1   \n",
      "3           0.522907     0.057606  ...    1       1                 1   \n",
      "4           0.079963     0.059441  ...    1       0                 1   \n",
      "\n",
      "   NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  \\\n",
      "0         34       20      28          119             0              124   \n",
      "1         50        9       8           39             0              217   \n",
      "2         10        2       7           42             2                5   \n",
      "3          3       27      15           22             1               31   \n",
      "4        244       15      34           72             1               85   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"PhiUSIIL_Phishing_URL_Dataset.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"ndarvind/phiusiil-phishing-url-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67974780",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "712cf846",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Let's select the most important features while removing highly correlated ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cbf3add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numeric features: ['URLLength', 'DomainLength', 'IsDomainIP', 'URLSimilarityIndex', 'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay', 'Crypto', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef', 'label']\n",
      "\n",
      "Features after removing leaky ones: ['URLLength', 'DomainLength', 'IsDomainIP', 'URLSimilarityIndex', 'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay', 'Crypto', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef']\n",
      "\n",
      "Highly correlated feature pairs:\n",
      "ObfuscationRatio - HasObfuscation: 0.799\n",
      "NoOfLettersInURL - URLLength: 0.956\n",
      "NoOfDegitsInURL - URLLength: 0.836\n",
      "NoOfDegitsInURL - NoOfObfuscatedChar: 0.721\n",
      "DegitRatioInURL - URLCharProb: -0.709\n",
      "NoOfEqualsInURL - NoOfObfuscatedChar: 0.755\n",
      "NoOfEqualsInURL - NoOfDegitsInURL: 0.806\n",
      "NoOfAmpersandInURL - NoOfObfuscatedChar: 0.786\n",
      "NoOfOtherSpecialCharsInURL - URLLength: 0.783\n",
      "NoOfOtherSpecialCharsInURL - NoOfDegitsInURL: 0.767\n",
      "NoOfOtherSpecialCharsInURL - NoOfEqualsInURL: 0.785\n",
      "SpacialCharRatioInURL - CharContinuationRate: -0.711\n",
      "URLTitleMatchScore - DomainTitleMatchScore: 0.961\n",
      "NoOfExternalRef - NoOfSelfRef: 0.701\n",
      "\n",
      "Final features after removing correlations: ['URLLength', 'DomainLength', 'IsDomainIP', 'URLSimilarityIndex', 'CharContinuationRate', 'TLDLegitimateProb', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'LetterRatioInURL', 'DegitRatioInURL', 'NoOfQMarkInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay', 'Crypto', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef']\n",
      "\n",
      "Cross-validation scores: [0.99994699 0.99994699 0.99992048 0.99997349 0.99997349]\n",
      "Average CV score: 0.9999522890927507\n",
      "\n",
      "Top 10 most important features after cleaning:\n",
      "               feature  importance\n",
      "3   URLSimilarityIndex    0.218461\n",
      "13          LineOfCode    0.174199\n",
      "34           NoOfImage    0.110026\n",
      "37         NoOfSelfRef    0.089209\n",
      "36              NoOfJS    0.070038\n",
      "26        HasSocialNet    0.066022\n",
      "35             NoOfCSS    0.052597\n",
      "33    HasCopyrightInfo    0.034811\n",
      "22      HasDescription    0.028215\n",
      "14   LargestLineLength    0.027750\n",
      "\n",
      "Classification Report with Cleaned Features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20189\n",
      "           1       1.00      1.00      1.00     26970\n",
      "\n",
      "    accuracy                           1.00     47159\n",
      "   macro avg       1.00      1.00      1.00     47159\n",
      "weighted avg       1.00      1.00      1.00     47159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Identify numeric and non-numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "print('Original numeric features:', list(numeric_cols))\n",
    "\n",
    "# Step 2: Remove leaky features that directly indicate phishing\n",
    "leaky_features = [\n",
    "    'IsPhishing', 'PhishingProb', 'NoOfPhishyCharacters',  # Direct indicators\n",
    "    'HasPhishingTerms', 'PhishyURLPattern',  # Likely derived from label\n",
    "    'IsBlacklisted', 'IsWhitelisted'  # Based on known phishing status\n",
    "]\n",
    "\n",
    "# Get initial feature set excluding leaky ones and the label\n",
    "feature_cols = [col for col in numeric_cols if col != 'label' and col not in leaky_features]\n",
    "print('\\nFeatures after removing leaky ones:', feature_cols)\n",
    "\n",
    "# Step 3: Find and handle highly correlated features\n",
    "X_initial = df[feature_cols]\n",
    "correlation_matrix = X_initial.corr()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "correlation_threshold = 0.7\n",
    "highly_correlated = []\n",
    "features_to_remove = set()\n",
    "\n",
    "# First, identify all highly correlated pairs\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            feat1, feat2 = correlation_matrix.columns[i], correlation_matrix.columns[j]\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            highly_correlated.append((feat1, feat2, corr))\n",
    "\n",
    "print('\\nHighly correlated feature pairs:')\n",
    "for feat1, feat2, corr in highly_correlated:\n",
    "    print(f'{feat1} - {feat2}: {corr:.3f}')\n",
    "\n",
    "# For each correlated pair, keep the one with higher variance\n",
    "for feat1, feat2, _ in highly_correlated:\n",
    "    var1 = X_initial[feat1].var()\n",
    "    var2 = X_initial[feat2].var()\n",
    "    if var1 >= var2:\n",
    "        features_to_remove.add(feat2)\n",
    "    else:\n",
    "        features_to_remove.add(feat1)\n",
    "\n",
    "# Get final feature set\n",
    "final_features = [f for f in feature_cols if f not in features_to_remove]\n",
    "print('\\nFinal features after removing correlations:', final_features)\n",
    "\n",
    "# Step 4: Prepare final dataset\n",
    "X = df[final_features]\n",
    "y = df['label']\n",
    "\n",
    "# Step 5: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 6: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 7: First evaluate with cross-validation\n",
    "rf_cv = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(rf_cv, X_train_scaled, y_train, cv=5)\n",
    "print('\\nCross-validation scores:', cv_scores)\n",
    "print('Average CV score:', cv_scores.mean())\n",
    "\n",
    "# Step 8: Train final model\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 9: Get feature importances from clean model\n",
    "importances = pd.DataFrame({\n",
    "    'feature': final_features,\n",
    "    'importance': rf_final.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values('importance', ascending=False)\n",
    "\n",
    "print('\\nTop 10 most important features after cleaning:')\n",
    "print(importances.head(10))\n",
    "\n",
    "# Step 10: Final evaluation\n",
    "y_pred = rf_final.predict(X_test_scaled)\n",
    "print('\\nClassification Report with Cleaned Features:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
