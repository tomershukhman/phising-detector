{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bc90abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b1/9mxt04xj57qg0q48b2b677cm0000gn/T/ipykernel_73865/4118202117.py:10: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:                                   URL  URLLength                      Domain  \\\n",
      "0    https://www.southbankmosaics.com         31    www.southbankmosaics.com   \n",
      "1            https://www.uni-mainz.de         23            www.uni-mainz.de   \n",
      "2      https://www.voicefmradio.co.uk         29      www.voicefmradio.co.uk   \n",
      "3         https://www.sfnmjournal.com         26         www.sfnmjournal.com   \n",
      "4  https://www.rewildingargentina.org         33  www.rewildingargentina.org   \n",
      "\n",
      "   DomainLength  IsDomainIP  TLD  URLSimilarityIndex  CharContinuationRate  \\\n",
      "0            24           0  com               100.0              1.000000   \n",
      "1            16           0   de               100.0              0.666667   \n",
      "2            22           0   uk               100.0              0.866667   \n",
      "3            19           0  com               100.0              1.000000   \n",
      "4            26           0  org               100.0              1.000000   \n",
      "\n",
      "   TLDLegitimateProb  URLCharProb  ...  Pay  Crypto  HasCopyrightInfo  \\\n",
      "0           0.522907     0.061933  ...    0       0                 1   \n",
      "1           0.032650     0.050207  ...    0       0                 1   \n",
      "2           0.028555     0.064129  ...    0       0                 1   \n",
      "3           0.522907     0.057606  ...    1       1                 1   \n",
      "4           0.079963     0.059441  ...    1       0                 1   \n",
      "\n",
      "   NoOfImage  NoOfCSS  NoOfJS  NoOfSelfRef  NoOfEmptyRef  NoOfExternalRef  \\\n",
      "0         34       20      28          119             0              124   \n",
      "1         50        9       8           39             0              217   \n",
      "2         10        2       7           42             2                5   \n",
      "3          3       27      15           22             1               31   \n",
      "4        244       15      34           72             1               85   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"PhiUSIIL_Phishing_URL_Dataset.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"ndarvind/phiusiil-phishing-url-dataset\",\n",
    "  file_path,\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")\n",
    "\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67974780",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "712cf846",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Let's select the most important features while removing highly correlated ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cbf3add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original numeric features: ['URLLength', 'DomainLength', 'IsDomainIP', 'URLSimilarityIndex', 'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay', 'Crypto', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef', 'label']\n",
      "\n",
      "Features after removing leaky ones: ['URLLength', 'DomainLength', 'IsDomainIP', 'CharContinuationRate', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio', 'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL', 'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'URLTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef', 'NoOfExternalRef']\n",
      "\n",
      "Highly correlated feature pairs:\n",
      "URLTitleMatchScore - DomainTitleMatchScore: 0.961\n",
      "NoOfLettersInURL - URLLength: 0.956\n",
      "NoOfDegitsInURL - URLLength: 0.836\n",
      "NoOfEqualsInURL - NoOfDegitsInURL: 0.806\n",
      "ObfuscationRatio - HasObfuscation: 0.799\n",
      "NoOfAmpersandInURL - NoOfObfuscatedChar: 0.786\n",
      "NoOfOtherSpecialCharsInURL - NoOfEqualsInURL: 0.785\n",
      "NoOfOtherSpecialCharsInURL - URLLength: 0.783\n",
      "NoOfOtherSpecialCharsInURL - NoOfDegitsInURL: 0.767\n",
      "NoOfEqualsInURL - NoOfObfuscatedChar: 0.755\n",
      "NoOfDegitsInURL - NoOfObfuscatedChar: 0.721\n",
      "SpacialCharRatioInURL - CharContinuationRate: -0.711\n",
      "NoOfExternalRef - NoOfSelfRef: 0.701\n",
      "\n",
      "Final features after removing correlations: ['URLLength', 'DomainLength', 'IsDomainIP', 'CharContinuationRate', 'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'LetterRatioInURL', 'DegitRatioInURL', 'NoOfQMarkInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef']\n",
      "\n",
      "Features removed due to suspicious distributions: ['IsDomainIP', 'IsDomainIP', 'HasObfuscation', 'HasObfuscation', 'DegitRatioInURL', 'NoOfSelfRedirect', 'HasExternalFormSubmit']\n",
      "\n",
      "Final feature set: ['URLLength', 'DomainLength', 'CharContinuationRate', 'TLDLength', 'NoOfSubDomain', 'NoOfObfuscatedChar', 'LetterRatioInURL', 'NoOfQMarkInURL', 'IsHTTPS', 'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive', 'NoOfURLRedirect', 'HasDescription', 'NoOfPopup', 'NoOfiFrame', 'HasSocialNet', 'HasSubmitButton', 'HasPasswordField', 'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef', 'NoOfEmptyRef']\n",
      "\n",
      "Cross-validation scores: [0.9994964  0.99944337 0.99952289 0.99949638 0.99954939]\n",
      "Average CV score: 0.9995016858144286\n",
      "\n",
      "Top 10 most important features:\n",
      "                  feature  importance\n",
      "9              LineOfCode    0.239104\n",
      "27            NoOfSelfRef    0.163955\n",
      "24              NoOfImage    0.143605\n",
      "20           HasSocialNet    0.103757\n",
      "26                 NoOfJS    0.102743\n",
      "23       HasCopyrightInfo    0.048678\n",
      "25                NoOfCSS    0.039754\n",
      "17         HasDescription    0.025937\n",
      "0               URLLength    0.024696\n",
      "12  DomainTitleMatchScore    0.020474\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20189\n",
      "           1       1.00      1.00      1.00     26970\n",
      "\n",
      "    accuracy                           1.00     47159\n",
      "   macro avg       1.00      1.00      1.00     47159\n",
      "weighted avg       1.00      1.00      1.00     47159\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Identify numeric and non-numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "print('Original numeric features:', list(numeric_cols))\n",
    "\n",
    "# Step 2: Remove potentially leaky features\n",
    "leaky_features = [\n",
    "    # Direct indicators of phishing\n",
    "    'IsPhishing', 'PhishingProb', 'NoOfPhishyCharacters',\n",
    "    'HasPhishingTerms', 'PhishyURLPattern',\n",
    "    'IsBlacklisted', 'IsWhitelisted',\n",
    "    # Features that might be derived from the label\n",
    "    'URLSimilarityIndex',  # Might be comparing against known phishing URLs\n",
    "    'TLDLegitimateProb',   # Might be based on known phishing TLDs\n",
    "    'URLCharProb',         # Might be based on phishing patterns\n",
    "    # Features that might leak label information\n",
    "    'HasHiddenFields',     # Strong indicator of phishing\n",
    "    'Bank', 'Pay', 'Crypto'  # Keywords likely used to create the dataset\n",
    "]\n",
    "\n",
    "# Get initial feature set excluding leaky ones and the label\n",
    "feature_cols = [col for col in numeric_cols if col != 'label' and col not in leaky_features]\n",
    "print('\\nFeatures after removing leaky ones:', feature_cols)\n",
    "\n",
    "# Step 3: Find and handle highly correlated features\n",
    "X_initial = df[feature_cols]\n",
    "correlation_matrix = X_initial.corr()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "correlation_threshold = 0.7\n",
    "highly_correlated = []\n",
    "features_to_remove = set()\n",
    "\n",
    "# First, identify all highly correlated pairs\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            feat1, feat2 = correlation_matrix.columns[i], correlation_matrix.columns[j]\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            highly_correlated.append((feat1, feat2, corr))\n",
    "\n",
    "print('\\nHighly correlated feature pairs:')\n",
    "for feat1, feat2, corr in sorted(highly_correlated, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f'{feat1} - {feat2}: {corr:.3f}')\n",
    "\n",
    "# For each correlated pair, keep the one with higher variance\n",
    "for feat1, feat2, _ in highly_correlated:\n",
    "    if feat1 not in features_to_remove and feat2 not in features_to_remove:\n",
    "        var1 = X_initial[feat1].var()\n",
    "        var2 = X_initial[feat2].var()\n",
    "        features_to_remove.add(feat2 if var1 >= var2 else feat1)\n",
    "\n",
    "# Get final feature set\n",
    "final_features = [f for f in feature_cols if f not in features_to_remove]\n",
    "print('\\nFinal features after removing correlations:', final_features)\n",
    "\n",
    "# Step 4: Check feature distributions and remove any suspicious ones\n",
    "X = df[final_features]\n",
    "suspicious_features = []\n",
    "for col in X.columns:\n",
    "    # Check for features with very low variance\n",
    "    if X[col].var() < 0.01:\n",
    "        suspicious_features.append(col)\n",
    "    # Check for features with suspicious value distributions\n",
    "    unique_vals = X[col].nunique()\n",
    "    if unique_vals <= 2 and X[col].value_counts().max() / len(X) > 0.95:\n",
    "        suspicious_features.append(col)\n",
    "\n",
    "final_features = [f for f in final_features if f not in suspicious_features]\n",
    "print('\\nFeatures removed due to suspicious distributions:', suspicious_features)\n",
    "print('\\nFinal feature set:', final_features)\n",
    "\n",
    "# Step 5: Prepare final dataset\n",
    "X = df[final_features]\n",
    "y = df['label']\n",
    "\n",
    "# Step 6: Split the data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 7: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 8: Train and evaluate with cross-validation\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=5)\n",
    "print('\\nCross-validation scores:', cv_scores)\n",
    "print('Average CV score:', cv_scores.mean())\n",
    "\n",
    "# Step 9: Train final model and get feature importances\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "importances = pd.DataFrame({\n",
    "    'feature': final_features,\n",
    "    'importance': rf.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values('importance', ascending=False)\n",
    "\n",
    "print('\\nTop 10 most important features:')\n",
    "print(importances.head(10))\n",
    "\n",
    "# Step 10: Evaluate on test set\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
